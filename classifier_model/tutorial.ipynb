{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMQMimaytq7-"
      },
      "source": [
        "# BERT Baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzGfg3dPtq8I"
      },
      "source": [
        "## DataThon @ IndoML'24\n",
        "\n",
        "This tutorial will walk you through a very simple BERT based baseline for the [DataThon](https://sites.google.com/view/datathon-indoml24) Challenge at IndoML'24.\n",
        "Feel free to play around with the repository, once you are done with this tutorial, it has been developed to work with multiple GPUs as well.\n",
        "At the end, you might also be able to make your first submission (albeit a very bad one!).\n",
        "\n",
        "### Table of Contents:\n",
        "\n",
        "1. [Preprocessing](#preprocessing)\n",
        "2. [Dataset Creation](#dataset-creation)\n",
        "3. [BERT Models](#bert)\n",
        "4. [Training](#training)\n",
        "5. [Evaluation](#evaluation)\n",
        "\n",
        "Please feel free to create a PR if you find any bugs or need help with running this code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWaHVbpPtq8L",
        "outputId": "957266b8-c956-47e1-c93e-24d91f6904f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'indoml-bert-baseline' already exists and is not an empty directory.\n",
            "Requirement already satisfied: numpy==1.23.5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (1.23.5)\n",
            "Requirement already satisfied: pandas==1.5.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (1.5.3)\n",
            "Requirement already satisfied: scikit_learn==1.2.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (1.2.1)\n",
            "Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (2.0.1)\n",
            "Requirement already satisfied: tqdm==4.64.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (4.64.1)\n",
            "Requirement already satisfied: transformers==4.33.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (4.33.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.3->-r requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.3->-r requirements.txt (line 2)) (2024.1)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit_learn==1.2.1->-r requirements.txt (line 3)) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit_learn==1.2.1->-r requirements.txt (line 3)) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit_learn==1.2.1->-r requirements.txt (line 3)) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r requirements.txt (line 4)) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r requirements.txt (line 4)) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r requirements.txt (line 4)) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r requirements.txt (line 4)) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r requirements.txt (line 4)) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r requirements.txt (line 4)) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r requirements.txt (line 4)) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r requirements.txt (line 4)) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r requirements.txt (line 4)) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r requirements.txt (line 4)) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r requirements.txt (line 4)) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r requirements.txt (line 4)) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r requirements.txt (line 4)) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r requirements.txt (line 4)) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r requirements.txt (line 4)) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r requirements.txt (line 4)) (11.7.91)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r requirements.txt (line 4)) (2.0.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.2->-r requirements.txt (line 6)) (0.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.2->-r requirements.txt (line 6)) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.2->-r requirements.txt (line 6)) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.2->-r requirements.txt (line 6)) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.2->-r requirements.txt (line 6)) (2.32.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.2->-r requirements.txt (line 6)) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.2->-r requirements.txt (line 6)) (0.4.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->-r requirements.txt (line 4)) (71.0.4)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->-r requirements.txt (line 4)) (0.44.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->-r requirements.txt (line 4)) (3.30.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->-r requirements.txt (line 4)) (18.1.8)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.33.2->-r requirements.txt (line 6)) (2024.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas==1.5.3->-r requirements.txt (line 2)) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1->-r requirements.txt (line 4)) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.33.2->-r requirements.txt (line 6)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.33.2->-r requirements.txt (line 6)) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.33.2->-r requirements.txt (line 6)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.33.2->-r requirements.txt (line 6)) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->-r requirements.txt (line 4)) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "# Please run this cell on colab, you can skip this if you are running locally.\n",
        "!git clone https://github.com/karannb/indoml-bert-baseline.git\n",
        "!cd indoml-bert-baseline/ && pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLokjrk5tq8O"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1O32TPMtq8O"
      },
      "source": [
        "Before preprocessing we need to get our data, please download it from [here](https://codalab.lisn.upsaclay.fr/competitions/19907#participate) after registering for the challenge.\n",
        "You can store it in any directory, but ideally store it in a new directory `data/`.\n",
        "Once you have the data running, the next cell will pre-process the data to have it in the format we have designed our `torch.utils.data.Dataset` objects, you can obviously change these as well.\n",
        "Feel free to got through the code if you want to change anything."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mk6t3NX_tq8P"
      },
      "source": [
        "### Preprocess and Categorize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2rvRM92xv5y",
        "outputId": "6a27d44a-c7a5-49e4-d63c-2d012a530370"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  input_data.zip\n",
            "replace indoml-bert-baseline/data/attribute_test.data? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: indoml-bert-baseline/data/attribute_test.data  \n",
            "  inflating: indoml-bert-baseline/data/attribute_train.data  \n",
            "  inflating: indoml-bert-baseline/data/attribute_train.solution  \n",
            "  inflating: indoml-bert-baseline/data/attribute_val.data  \n",
            "  inflating: indoml-bert-baseline/data/attribute_val.solution  \n"
          ]
        }
      ],
      "source": [
        "!unzip input_data.zip -d indoml-bert-baseline/data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/rec-data/sapta/misc/phase_2/indoml-bert-baseline\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtupLoyUzGdk",
        "outputId": "0bc2b887-9a2b-4221-b2be-09cf3c4bad90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'indoml-bert-baseline/'\n",
            "/rec-data/sapta/misc/phase_2/indoml-bert-baseline\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/rec-data/sapta/misc/env_bert/lib/python3.10/site-packages/IPython/core/magics/osm.py:393: UserWarning: This is now an optional IPython functionality, using bookmarks requires you to install the `pickleshare` library.\n",
            "  bkms = self.shell.db.get('bookmarks', {})\n"
          ]
        }
      ],
      "source": [
        "%cd indoml-bert-baseline/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IYtQe0Xtq8P",
        "outputId": "37bf83fa-ccb8-44bb-9259-04ae91d980f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessing data/attribute_test.data, will store in data/test.json ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "184664it [00:00, 368751.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessing data/attribute_val_stratified.data, will store in data/val.json ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "89885it [00:00, 383234.88it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessing data/attribute_val_stratified.solution, will store in data/val_sol.json ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "89885it [00:00, 385462.05it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessing data/attribute_train_stratified.data, will store in data/train.json ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "471953it [00:01, 427046.34it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessing data/attribute_train_stratified.solution, will store in data/train_sol.json ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "471953it [00:01, 372601.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attribute: supergroup has 32 unique values.\n",
            "Maps for supergroup already exist, skipped.\n",
            "Attribute: group has 227 unique values.\n",
            "Maps for group already exist, skipped.\n",
            "Attribute: module has 441 unique values.\n",
            "Maps for module already exist, skipped.\n",
            "Attribute: brand has 3974 unique values.\n",
            "Maps for brand already exist, skipped.\n"
          ]
        }
      ],
      "source": [
        "from src.preprocess import preprocess_fn, categorize\n",
        "\n",
        "# this will preprocess the data to a particular format (in json)\n",
        "preprocess_fn(\"attribute_test.data\", \"data\")\n",
        "preprocess_fn(\"attribute_val_stratified.data\", \"data\")\n",
        "preprocess_fn(\"attribute_val_stratified.solution\", \"data\")\n",
        "preprocess_fn(\"attribute_train_stratified.data\", \"data\")\n",
        "preprocess_fn(\"attribute_train_stratified.solution\", \"data\")\n",
        "\n",
        "\n",
        "# this will create maps of index2class and class2index for all columns or outputs\n",
        "for col in ['supergroup', 'group', 'module', 'brand']:\n",
        "    categorize(col)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sr65JTd5tq8Q"
      },
      "source": [
        "## Dataset Creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBvgDg8Utq8Q"
      },
      "source": [
        "Now we will create PyTorch Datasets for convinient DataLoading (across multiple devices as well).\n",
        "We would strongly recommend you to start modifying stuff here in case you want to use a BERT baseline itself, we have also added a `TODO: CAN CHANGE HERE` where we thought modifications are possible.\n",
        "DO NOT USE `trim=True` when you are training your own model! It is only helpful for debugging purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gG3j8wHLtq8R",
        "outputId": "a1393759-6d72-4bc0-a360-ee99ea5dd283"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading val data: 100%|██████████| 89885/89885 [00:00<00:00, 1584947.05it/s]\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "'store'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "File \u001b[0;32m/rec-data/sapta/misc/env_bert/lib/python3.10/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
            "File \u001b[0;32m/rec-data/sapta/misc/env_bert/lib/python3.10/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32m/rec-data/sapta/misc/env_bert/lib/python3.10/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'store'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[15], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ReviewsDataset, ReviewsDataLoader\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# this is just a simple interface run\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mReviewsDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mval\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msupergroup\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(dataset[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m      7\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m ReviewsDataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "File \u001b[0;32m/rec-data/sapta/misc/phase_2/indoml-bert-baseline/src/dataset.py:54\u001b[0m, in \u001b[0;36mReviewsDataset.__init__\u001b[0;34m(self, data_dir, split, output, trim)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProduct Name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Add 'retailer' and 'price' information if they are not 'None'\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, Sold by retailer: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretailer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretailer\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     56\u001b[0m )\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprice\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, Price: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mloc[\n\u001b[1;32m     60\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprice\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprice\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     61\u001b[0m     ]\n\u001b[1;32m     62\u001b[0m )\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# # trim the dataset for debugging\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# if trim:\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m#     if split == \"train\":\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m#         idx = np.random.choice(len(self.data), 200, replace=False)\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m#         self.data = self.data.iloc[idx]\u001b[39;00m\n",
            "File \u001b[0;32m/rec-data/sapta/misc/env_bert/lib/python3.10/site-packages/pandas/core/frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
            "File \u001b[0;32m/rec-data/sapta/misc/env_bert/lib/python3.10/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
            "\u001b[0;31mKeyError\u001b[0m: 'store'"
          ]
        }
      ],
      "source": [
        "from src.dataset import ReviewsDataset, ReviewsDataLoader\n",
        "\n",
        "# this is just a simple interface run\n",
        "dataset = ReviewsDataset(data_dir=\"data/\", split=\"val\", output=\"supergroup\", trim=False)\n",
        "print(dataset[0])\n",
        "\n",
        "dataloader = ReviewsDataLoader(dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "for batch in dataloader:\n",
        "    print(batch)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9Sl2J6Rtq8S"
      },
      "source": [
        "## BERT Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3dOad42tq8S"
      },
      "source": [
        "This is a simple script to download a BERT model ans it's tokenizer, you can also play around with different types of BERT models from HuggingFace.\n",
        "We stick to the basic one [here](https://huggingface.co/google-bert/bert-base-uncased).\n",
        "Please save them appropriately, or modify the path in the `trainer.py`, by default it should be saved in `bert-model/` and `bert-tokenizer/`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmrPShhKtq8S",
        "outputId": "b7fba0fd-67cf-4d36-ffde-e911ab946eeb"
      },
      "outputs": [],
      "source": [
        "from src.downloadBERT import download\n",
        "\n",
        "# this function will download the model and tokenizer\n",
        "download()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTERJiy3tq8T"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BEO7FR8tq8T"
      },
      "source": [
        "Now the interesting part, training.\n",
        "Note that there are **several** parameters that can be modified here, from learning rate to batch size to the optimizer used, weight decay, etc..\n",
        "Apart from that, the code also doesn't have checkpointing as of now, you will probably need that.\n",
        "You can also change the criteria for best model selection (currently f1-score).\n",
        "Again, remember to NOT TRIM when you are actually training your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkXJ19lktq8T",
        "outputId": "d72cadef-73e0-4790-b5e8-23549cea74de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on cuda\n",
            "Using device: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading train data: 100%|██████████| 443499/443499 [00:00<00:00, 678456.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data loaded in 0.064 minutes.\n",
            "train data has 1000 examples.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading val data: 100%|██████████| 95035/95035 [00:00<00:00, 566259.97it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data loaded in 0.013 minutes.\n",
            "val data has 200 examples.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading test data: 100%|██████████| 95036/95036 [00:00<00:00, 753410.44it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data loaded in 0.008 minutes.\n",
            "test data has 95036 examples.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./bert-model/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 125/125 [00:32<00:00,  3.85it/s]\n",
            "100%|██████████| 25/25 [00:06<00:00,  3.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.0950, Precision: 0.9289, Recall: 0.1160, F1: 0.0516\n",
            "Epoch 2/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 125/125 [00:31<00:00,  3.95it/s]\n",
            "100%|██████████| 25/25 [00:06<00:00,  4.05it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.1650, Precision: 0.7659, Recall: 0.3190, F1: 0.0938\n",
            "Epoch 3/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 125/125 [00:31<00:00,  3.96it/s]\n",
            "100%|██████████| 25/25 [00:06<00:00,  3.94it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.1900, Precision: 0.7228, Recall: 0.3806, F1: 0.1170\n",
            "Epoch 4/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 125/125 [00:31<00:00,  3.93it/s]\n",
            "100%|██████████| 25/25 [00:06<00:00,  3.99it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.2250, Precision: 0.7560, Recall: 0.3648, F1: 0.1421\n",
            "Epoch 5/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 125/125 [00:31<00:00,  3.96it/s]\n",
            "100%|██████████| 25/25 [00:06<00:00,  3.98it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.2450, Precision: 0.7156, Recall: 0.4237, F1: 0.1502\n",
            "Using device: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading train data: 100%|██████████| 443499/443499 [00:00<00:00, 514507.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data loaded in 0.073 minutes.\n",
            "train data has 1000 examples.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading val data: 100%|██████████| 95035/95035 [00:00<00:00, 609729.34it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data loaded in 0.013 minutes.\n",
            "val data has 200 examples.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading test data: 100%|██████████| 95036/95036 [00:00<00:00, 681699.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data loaded in 0.007 minutes.\n",
            "test data has 95036 examples.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./bert-model/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 125/125 [00:31<00:00,  3.93it/s]\n",
            "100%|██████████| 25/25 [00:06<00:00,  3.91it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.5500, Precision: 0.7930, Recall: 0.3432, F1: 0.3255\n",
            "Epoch 2/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 125/125 [00:31<00:00,  3.98it/s]\n",
            "100%|██████████| 25/25 [00:06<00:00,  4.00it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.6550, Precision: 0.8066, Recall: 0.4700, F1: 0.4880\n",
            "Epoch 3/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 125/125 [00:31<00:00,  3.99it/s]\n",
            "100%|██████████| 25/25 [00:06<00:00,  3.94it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.6200, Precision: 0.7147, Recall: 0.4003, F1: 0.3874\n",
            "Epoch 4/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 125/125 [00:31<00:00,  3.95it/s]\n",
            "100%|██████████| 25/25 [00:06<00:00,  3.96it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.6900, Precision: 0.7597, Recall: 0.4836, F1: 0.4702\n",
            "Epoch 5/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 125/125 [00:31<00:00,  3.97it/s]\n",
            "100%|██████████| 25/25 [00:06<00:00,  3.97it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.7100, Precision: 0.7376, Recall: 0.5806, F1: 0.5775\n",
            "Using device: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading train data: 100%|██████████| 443499/443499 [00:00<00:00, 696781.47it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data loaded in 0.063 minutes.\n",
            "train data has 1000 examples.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading val data: 100%|██████████| 95035/95035 [00:00<00:00, 579742.86it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data loaded in 0.012 minutes.\n",
            "val data has 200 examples.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading test data: 100%|██████████| 95036/95036 [00:00<00:00, 789456.24it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data loaded in 0.008 minutes.\n",
            "test data has 95036 examples.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./bert-model/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 125/125 [00:31<00:00,  3.95it/s]\n",
            "100%|██████████| 25/25 [00:06<00:00,  3.93it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.3550, Precision: 0.8312, Recall: 0.1777, F1: 0.1048\n",
            "Epoch 2/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 125/125 [00:31<00:00,  3.96it/s]\n",
            "100%|██████████| 25/25 [00:06<00:00,  3.99it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.4400, Precision: 0.7704, Recall: 0.2859, F1: 0.2131\n",
            "Epoch 3/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 125/125 [00:31<00:00,  3.98it/s]\n",
            "100%|██████████| 25/25 [00:06<00:00,  3.95it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.4850, Precision: 0.8116, Recall: 0.3562, F1: 0.2911\n",
            "Epoch 4/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 125/125 [00:31<00:00,  3.95it/s]\n",
            "100%|██████████| 25/25 [00:06<00:00,  3.95it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.4900, Precision: 0.8210, Recall: 0.3085, F1: 0.2704\n",
            "Epoch 5/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 125/125 [00:31<00:00,  3.96it/s]\n",
            "100%|██████████| 25/25 [00:06<00:00,  3.96it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.5300, Precision: 0.7769, Recall: 0.4116, F1: 0.3406\n",
            "Using device: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading train data: 100%|██████████| 443499/443499 [00:00<00:00, 677091.58it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data loaded in 0.060 minutes.\n",
            "train data has 1000 examples.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading val data: 100%|██████████| 95035/95035 [00:00<00:00, 583853.70it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data loaded in 0.012 minutes.\n",
            "val data has 200 examples.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading test data: 100%|██████████| 95036/95036 [00:00<00:00, 455465.13it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data loaded in 0.011 minutes.\n",
            "test data has 95036 examples.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./bert-model/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 125/125 [00:31<00:00,  3.94it/s]\n",
            "100%|██████████| 25/25 [00:06<00:00,  3.89it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.2250, Precision: 0.8527, Recall: 0.1659, F1: 0.0904\n",
            "Epoch 2/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 125/125 [00:31<00:00,  3.97it/s]\n",
            "100%|██████████| 25/25 [00:06<00:00,  4.00it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.2600, Precision: 0.8216, Recall: 0.2291, F1: 0.1346\n",
            "Epoch 3/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 125/125 [00:31<00:00,  3.98it/s]\n",
            "100%|██████████| 25/25 [00:06<00:00,  3.96it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.3350, Precision: 0.8145, Recall: 0.3064, F1: 0.1965\n",
            "Epoch 4/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 125/125 [00:31<00:00,  3.95it/s]\n",
            "100%|██████████| 25/25 [00:06<00:00,  3.94it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.3250, Precision: 0.7713, Recall: 0.3138, F1: 0.1837\n",
            "Epoch 5/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 125/125 [00:31<00:00,  3.97it/s]\n",
            "100%|██████████| 25/25 [00:06<00:00,  3.96it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.3650, Precision: 0.6890, Recall: 0.3888, F1: 0.2206\n",
            "Using device: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading train data: 100%|██████████| 443499/443499 [00:00<00:00, 709655.52it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 'na' as the label, removing examples with 'na' label.\n",
            "Original data has 443499 examples.\n",
            "Data loaded in 0.065 minutes.\n",
            "train data has 1000 examples.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading val data: 100%|██████████| 95035/95035 [00:00<00:00, 580503.57it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 'na' as the label, removing examples with 'na' label.\n",
            "Original data has 95035 examples.\n",
            "Data loaded in 0.013 minutes.\n",
            "val data has 200 examples.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading test data: 100%|██████████| 95036/95036 [00:00<00:00, 734028.69it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data loaded in 0.007 minutes.\n",
            "test data has 95036 examples.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./bert-model/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 125/125 [00:31<00:00,  3.94it/s]\n",
            "100%|██████████| 25/25 [00:06<00:00,  3.90it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.0800, Precision: 0.9334, Recall: 0.0793, F1: 0.0399\n",
            "Epoch 2/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 125/125 [00:31<00:00,  3.96it/s]\n",
            "100%|██████████| 25/25 [00:06<00:00,  4.00it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.1200, Precision: 0.8208, Recall: 0.1733, F1: 0.0469\n",
            "Epoch 3/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 125/125 [00:31<00:00,  3.97it/s]\n",
            "100%|██████████| 25/25 [00:06<00:00,  3.91it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.1700, Precision: 0.7229, Recall: 0.2894, F1: 0.0784\n",
            "Epoch 4/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 125/125 [00:31<00:00,  3.95it/s]\n",
            "100%|██████████| 25/25 [00:06<00:00,  3.94it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.2450, Precision: 0.7181, Recall: 0.3311, F1: 0.1062\n",
            "Epoch 5/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 125/125 [00:31<00:00,  3.97it/s]\n",
            "100%|██████████| 25/25 [00:06<00:00,  3.97it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.1950, Precision: 0.7060, Recall: 0.3014, F1: 0.1033\n",
            "Using device: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading train data: 100%|██████████| 443499/443499 [00:00<00:00, 679673.91it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 'na' as the label, removing examples with 'na' label.\n",
            "Original data has 443499 examples.\n",
            "Data loaded in 0.058 minutes.\n",
            "train data has 1000 examples.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading val data: 100%|██████████| 95035/95035 [00:00<00:00, 534864.57it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 'na' as the label, removing examples with 'na' label.\n",
            "Original data has 95035 examples.\n",
            "Data loaded in 0.012 minutes.\n",
            "val data has 200 examples.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading test data: 100%|██████████| 95036/95036 [00:00<00:00, 704250.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data loaded in 0.008 minutes.\n",
            "test data has 95036 examples.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./bert-model/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 125/125 [00:31<00:00,  3.94it/s]\n",
            "100%|██████████| 25/25 [00:06<00:00,  3.92it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.1200, Precision: 0.9232, Recall: 0.0897, F1: 0.0376\n",
            "Epoch 2/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 125/125 [00:31<00:00,  3.96it/s]\n",
            "100%|██████████| 25/25 [00:06<00:00,  4.01it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.1950, Precision: 0.8193, Recall: 0.2324, F1: 0.1028\n",
            "Epoch 3/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 125/125 [00:31<00:00,  3.97it/s]\n",
            "100%|██████████| 25/25 [00:06<00:00,  3.96it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.2500, Precision: 0.7281, Recall: 0.3413, F1: 0.1329\n",
            "Epoch 4/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 125/125 [00:31<00:00,  3.95it/s]\n",
            "100%|██████████| 25/25 [00:06<00:00,  3.99it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.3450, Precision: 0.7118, Recall: 0.4087, F1: 0.2040\n",
            "Epoch 5/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 125/125 [00:31<00:00,  3.98it/s]\n",
            "100%|██████████| 25/25 [00:06<00:00,  3.99it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.2950, Precision: 0.6880, Recall: 0.3940, F1: 0.1797\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from src.trainer import Trainer\n",
        "\n",
        "# note that this code IS NOT going to run on multiple GPUs,\n",
        "# please see the README / trainer.py for that. If you run\n",
        "# on colab, it will utilize the GPU there. Please raise an\n",
        "# issue if you don't see that happen.\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Running on {device}\")\n",
        "\n",
        "trainers = {}\n",
        "for col in ['supergroup', 'group', 'module', 'brand']:\n",
        "    print(\"*\"*88)\n",
        "    print(f\"Training for {col}\")\n",
        "    print(\"*\"*88)\n",
        "    trainers[col] = Trainer(data_dir=\"data/\", device=device, output=col, trim=True)\n",
        "    trainers[col].train()\n",
        "    print(f\"Training finished for {col}\")\n",
        "    print(\"*\"*88)\n",
        "    # print(f\"Testing on {col}\")\n",
        "    # print(\"*\"*88)\n",
        "    # trainer[col].test()\n",
        "    # print(f\"Testing finished for {col}\")\n",
        "    # print(\"*\"*88)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsSTphSUtq8U"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfQ8qZWetq8W"
      },
      "source": [
        "We have also provided utility evaluation functions (which we use for validation), to judge your model.\n",
        "Please note that if you want to perform model selection based on `item accuracy` (the main metric for the contest), since we are training 6 different models for 6 different columns, to get the `item accuracy` you need to store results from the validation run how we have done for the test run.\n",
        "To make a valid submission, just run the previous code cell with the testing lines uncommented and then you can use the next cell to generate a valid submission. (NOTE: this takes\n",
        "very long, as the test output must be on the entire test dataset ~49 minutes on one GPU on colab, you can reduce this by using a bigger batch size probably)\n",
        "The next cell will generate a zip file that you can upload to CodaLab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ohemJY9MTFB"
      },
      "outputs": [],
      "source": [
        "from src.evaluate import postprocess\n",
        "\n",
        "# postprocess()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjZmMWkGtq8Z"
      },
      "source": [
        "## Thank you!\n",
        "Repository and tutorial by [Karan Bania](https://karannb.github.io)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env_bert",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
